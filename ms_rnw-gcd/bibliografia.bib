@misc{mitlib2023,
    Author = {{The MIT Libraries}},
    Howpublished = {\url{https://libguides.mit.edu/c.php?g=176032&p=1159439}},
    note = {Último acceso el 06 de noviembre de 2023},
    Month = {January},
    Title = {Overview - Citing sources - LibGuides at MIT Libraries},
    Url = {https://libguides.mit.edu/c.php?g=176032&p=1159439},
    Urldate = {Nov 6, 2023},
    Year = {2023},
}

@article{Cutler2020,
    author = {Cutler, David M. and Summers, Lawrence H.},
    title = "{The COVID-19 Pandemic and the \$16 Trillion Virus}",
    journal = {JAMA},
    volume = {324},
    number = {15},
    pages = {1495-1496},
    year = {2020},
    month = {10},
    abstract = "{The SARS-CoV-2 (severe acute respiratory syndrome coronavirus 2) pandemic is the greatest threat to prosperity and well-being the US has encountered since the Great Depression. This Viewpoint aggregates mortality, morbidity, mental health conditions, and direct economic losses to estimate the total cost of the pandemic in the US on the optimistic assumption that it will be substantially contained by the fall of 2021. These costs far exceed those associated with conventional recessions and the Iraq War, and are similar to those associated with global climate change. However, increased investment in testing and contact tracing could have economic benefits that are at least 30 times greater than the estimated costs of the investment in these approaches.}",
    issn = {0098-7484},
    doi = {10.1001/jama.2020.19759},
    url = {https://doi.org/10.1001/jama.2020.19759},
    eprint = {https://jamanetwork.com/journals/jama/articlepdf/2771764/jama\_cutler\_2020\_vp\_200215\_1602876140.96328.pdf},
}

@article{Deaton2018,
title = {Understanding and misunderstanding randomized controlled trials},
journal = {Social Science \& Medicine},
volume = {210},
pages = {2-21},
year = {2018},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2017.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0277953617307359},
author = {Angus Deaton and Nancy Cartwright},
keywords = {RCTs, Balance, Bias, Precision, External validity, Transportation of results, Health, Economic development},
abstract = {Randomized Controlled Trials (RCTs) are increasingly popular in the social sciences, not only in medicine. We argue that the lay public, and sometimes researchers, put too much trust in RCTs over other methods of investigation. Contrary to frequent claims in the applied literature, randomization does not equalize everything other than the treatment in the treatment and control groups, it does not automatically deliver a precise estimate of the average treatment effect (ATE), and it does not relieve us of the need to think about (observed or unobserved) covariates. Finding out whether an estimate was generated by chance is more difficult than commonly believed. At best, an RCT yields an unbiased estimate, but this property is of limited practical value. Even then, estimates apply only to the sample selected for the trial, often no more than a convenience sample, and justification is required to extend the results to other groups, including any population to which the trial sample belongs, or to any individual, including an individual in the trial. Demanding ‘external validity’ is unhelpful because it expects too much of an RCT while undervaluing its potential contribution. RCTs do indeed require minimal assumptions and can operate with little prior knowledge. This is an advantage when persuading distrustful audiences, but it is a disadvantage for cumulative scientific progress, where prior knowledge should be built upon, not discarded. RCTs can play a role in building scientific knowledge and useful predictions but they can only do so as part of a cumulative program, combining with other methods, including conceptual and theoretical development, to discover not ‘what works’, but ‘why things work’.}
}

@book{Angrist2015,
  author = {Angrist, Joshua and Pischke, Jörn-Steffen},
  biburl = {https://www.bibsonomy.org/bibtex/27b9f36f67e9372e7606eb255a8ef0c02/mayeehab},
  interhash = {d083248c804cc177bb55a60599f671a1},
  intrahash = {7b9f36f67e9372e7606eb255a8ef0c02},
  keywords = {commuting imported},
  publisher = {Princeton University Press},
  timestamp = {2018-02-12T13:50:53.000+0100},
  title = {Mastering 'Metrics: The path from cause to effect},
  type = {Book},
  year = 2015
}


@article{Betsch2020,
author = {Cornelia Betsch  and Lars Korn  and Philipp Sprengholz  and Lisa Felgendreff  and Sarah Eitze  and Philipp Schmid  and Robert Böhm },
title = {Social and behavioral consequences of mask policies during the COVID-19 pandemic},
journal = {Proceedings of the National Academy of Sciences},
volume = {117},
number = {36},
pages = {21851-21853},
year = {2020},
doi = {10.1073/pnas.2011674117},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2011674117},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2011674117},
abstract = {Mandatory and voluntary mask policies may have yet unknown social and behavioral consequences related to the effectiveness of the measure, stigmatization, and perceived fairness. Serial cross-sectional data (April 14 to May 26, 2020) from nearly 7,000 German participants demonstrate that implementing a mandatory policy increased actual compliance despite moderate acceptance; mask wearing correlated positively with other protective behaviors. A preregistered experiment (n = 925) further indicates that a voluntary policy would likely lead to insufficient compliance, would be perceived as less fair, and could intensify stigmatization. A mandatory policy appears to be an effective, fair, and socially responsible solution to curb transmissions of airborne viruses.}}


@article{McDermott2020,
author = {Rose McDermott  and Peter K. Hatemi },
title = {Ethics in field experimentation: A call to establish new standards to protect the public from unwanted manipulation and real harms},
journal = {Proceedings of the National Academy of Sciences},
volume = {117},
number = {48},
pages = {30014-30021},
year = {2020},
doi = {10.1073/pnas.2012021117},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2012021117},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2012021117},
abstract = {In 1966, Henry Beecher published his foundational paper “Ethics and Clinical Research,” bringing to light unethical experiments that were routinely being conducted by leading universities and government agencies. A common theme was the lack of voluntary consent. Research regulations surrounding laboratory experiments flourished after his work. More than half a century later, we seek to follow in his footsteps and identify a new domain of risk to the public: certain types of field experiments. The nature of experimental research has changed greatly since the Belmont Report. Due in part to technological advances including social media, experimenters now target and affect whole societies, releasing interventions into a living public, often without sufficient review or controls. A large number of social science field experiments do not reflect compliance with current ethical and legal requirements that govern research with human participants. Real-world interventions are being conducted without consent or notice to the public they affect. Follow-ups and debriefing are routinely not being undertaken with the populations that experimenters injure. Importantly, even when ethical research guidelines are followed, researchers are following principles developed for experiments in controlled settings, with little assessment or protection for the wider societies within which individuals are embedded. We strive to improve the ethics of future work by advocating the creation of new norms, illustrating classes of field experiments where scholars do not appear to have recognized the ways such research circumvents ethical standards by putting people, including those outside the manipulated group, into harm’s way.}}

@book{LATEX,
title={Composición de textos científicos con LATEX},
ISBN={9788498800517}, 
url={http://hdl.handle.net/2099.3/36359},
publisher={Aula teòrica (61), Edicions UPC}, 
author={Valiente Feruglio, Gabriel Alejandro}, 
year={1997}}

@article{ggplot2,
 title={ggplot2 - Elegant Graphics for Data Analysis (2nd Edition)},
 volume={77},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v077b02},
 doi={10.18637/jss.v077.b02},
 number={2},
 journal={Journal of Statistical Software, Book Reviews},
 author={Gómez-Rubio, Virgilio},
 year={2017}
 %pages={1–3}
}


@article{Vuorre2018,
   abstract = {Recent calls for improving reproducibility have increased attention to the ways in which researchers curate, share, and collaborate on their research assets. In this Tutorial, we explain how version control systems, such as the popular Git program, support these functions and then show how to use Git with a graphical interface in the RStudio program. This Tutorial is written for researchers with no previous experience using version control systems and covers both single-user and collaborative workflows. The online Supplemental Material provides information on advanced Git command-line functions. Git presents an elegant solution to specific challenges to curating, sharing, and collaborating on research assets and can be implemented in common workflows with little extra effort.},
   author = {Matti Vuorre and James P. Curley},
   doi = {10.1177/2515245918754826},
   issn = {25152467},
   issue = {2},
   journal = {Advances in Methods and Practices in Psychological Science},
   title = {Curating Research Assets: A Tutorial on the Git Version Control System},
   volume = {1},
   year = {2018},
}

@article{Smith2022,
   abstract = {The main purpose of many medical studies is to estimate the effects of a treatment or exposure on an outcome. However, it is not always possible to randomize the study participants to a particular treatment, therefore observational study designs may be used. There are major challenges with observational studies; one of which is confounding. Controlling for confounding is commonly performed by direct adjustment of measured confounders; although, sometimes this approach is suboptimal due to modeling assumptions and misspecification. Recent advances in the field of causal inference have dealt with confounding by building on classical standardization methods. However, these recent advances have progressed quickly with a relative paucity of computational-oriented applied tutorials contributing to some confusion in the use of these methods among applied researchers. In this tutorial, we show the computational implementation of different causal inference estimators from a historical perspective where new estimators were developed to overcome the limitations of the previous estimators (ie, nonparametric and parametric g-formula, inverse probability weighting, double-robust, and data-adaptive estimators). We illustrate the implementation of different methods using an empirical example from the Connors study based on intensive care medicine, and most importantly, we provide reproducible and commented code in Stata, R, and Python for researchers to adapt in their own observational study. The code can be accessed at https://github.com/migariane/Tutorial_Computational_Causal_Inference_Estimators.},
   author = {Matthew J. Smith and Mohammad A. Mansournia and Camille Maringe and Paul N. Zivich and Stephen R. Cole and Clémence Leyrat and Aurélien Belot and Bernard Rachet and Miguel A. Luque-Fernandez},
   doi = {10.1002/sim.9234},
   issn = {10970258},
   issue = {2},
   journal = {Statistics in Medicine},
   title = {Introduction to computational causal inference using reproducible Stata, R, and Python code: A tutorial},
   volume = {41},
   year = {2022},
}

@article{Wing2018,
   abstract = {The difference in difference (DID) design is a quasi-experimental research design that researchers often use to study causal relationships in public health settings where randomized controlled trials (RCTs) are infeasible or unethical. However, causal inference poses many challenges in DID designs. In this article, we review key features of DID designs with an emphasis on public health policy research. Contemporary researchers should take an active approach to the design ofDIDstudies, seeking to construct comparison groups, sensitivity analyses, and robustness checks that help validate the method's assumptions. We explain the key assumptions of the design and discuss analytic tactics, supplementary analysis, and approaches to statistical inference that are often important in applied research. The DID design is not a perfect substitute for randomized experiments, but it often represents a feasible way to learn about casual relationships. We conclude by noting that combining elements from multiple quasi-experimental techniques may be important in the next wave of innovations to the DID approach.},
   author = {Coady Wing and Kosali Simon and Ricardo A. Bello-Gomez},
   doi = {10.1146/annurev-publhealth-040617-013507},
   issn = {15452093},
   journal = {Annual Review of Public Health},
   title = {Designing Difference in Difference Studies: Best Practices for Public Health Policy Research},
   volume = {39},
   year = {2018},
}

@article{Callaway2021,
   abstract = {In this article, we consider identification, estimation, and inference procedures for treatment effect parameters using Difference-in-Differences (DiD) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the “parallel trends assumption” holds potentially only after conditioning on observed covariates. We show that a family of causal effect parameters are identified in staggered DiD setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. Our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. We also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. We establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001–2007. Open-source software is available for implementing the proposed methods.},
   author = {Brantly Callaway and Pedro H.C. Sant'Anna},
   doi = {10.1016/j.jeconom.2020.12.001},
   issn = {18726895},
   issue = {2},
   journal = {Journal of Econometrics},
   title = {Difference-in-Differences with multiple time periods},
   volume = {225},
   year = {2021},
}
@article{Baker2022,
   abstract = {We explain when and how staggered difference-in-differences regression estimators, commonly applied to assess the impact of policy changes, are biased. These biases are likely to be relevant for a large portion of research settings in finance, accounting, and law that rely on staggered treatment timing, and can result in Type-I and Type-II errors. We summarize three alternative estimators developed in the econometrics and applied literature for addressing these biases, including their differences and tradeoffs. We apply these estimators to re-examine prior published results and show, in many cases, the alternative causal estimates or inferences differ substantially from prior papers.},
   author = {Andrew C. Baker and David F. Larcker and Charles C.Y. Wang},
   doi = {10.1016/j.jfineco.2022.01.004},
   issn = {0304405X},
   issue = {2},
   journal = {Journal of Financial Economics},
   title = {How much should we trust staggered difference-in-differences estimates?},
   volume = {144},
   year = {2022},
}
@article{Porreca2022,
   abstract = {This note formalizes the synthetic difference-in-differences estimator for staggered treatment adoption settings, as briefly described in Arkhangelsky et al. (2021). To illustrate the importance of this estimator, I use replication data from Abrams (2012). I compare the estimators obtained using SynthDiD, TWFE, the group time average treatment effect estimator of Callaway and Sant'Anna (2021), and the partially pooled synthetic control method estimator of Ben-Michael et al. (2021) in a staggered treatment adoption setting. I find that in this staggered treatment setting, SynthDiD provides a numerically different estimate of the average treatment effect. Simulation results show that these differences may be attributable to the underlying data generating process more closely mirroring that of the latent factor model assumed for SynthDiD than that of additive fixed effects assumed under traditional difference-in-differences frameworks.},
   author = {Zachary Porreca},
   doi = {10.1016/j.econlet.2022.110874},
   issn = {01651765},
   journal = {Economics Letters},
   title = {Synthetic difference-in-differences estimation with staggered treatment timing},
   volume = {220},
   year = {2022},
}
@article{Cowger2022,
   abstract = {Abstract Background In February 2022, Massachusetts rescinded a statewide universal masking policy in public schools, and many Massachusetts school districts lifted masking requirements during the ...},
   author = {Tori L. Cowger and Eleanor J. Murray and Jaylen Clarke and Mary T. Bassett and Bisola O. Ojikutu and Sarimer M. Sánchez and Natalia Linos and Kathryn T. Hall},
   doi = {10.1056/nejmoa2211029},
   issn = {0028-4793},
   issue = {21},
   journal = {New England Journal of Medicine},
   title = {Lifting Universal Masking in Schools — Covid-19 Incidence among Students and Staff},
   volume = {387},
   year = {2022},
}

@misc{Cowger2022-supp,
   author = {Tori L. Cowger and Eleanor J. Murray and Jaylen Clarke and Mary 
T. Bassett and Bisola O. Ojikutu and Sarimer M. Sánchez and Natalia Linos and 
Kathryn T. Hall},
   doi = {10.1056/nejmoa2211029},
   issn = {0028-4793},
   issue = {21},
   journal = {New England Journal of Medicine},
   title = {Supplement to ``{L}ifting Universal Masking in Schools — 
Covid-19 Incidence among Students and Staff''},
   volume = {387},
   year = {2022},
}

@misc{Cowger2022-plan,
   author = {Tori L. Cowger and Eleanor J. Murray and Jaylen Clarke and Mary 
T. Bassett and Bisola O. Ojikutu and Sarimer M. Sánchez and Natalia Linos and 
Kathryn T. Hall},
   doi = {10.1056/nejmoa2211029},
   issn = {0028-4793},
   issue = {21},
   journal = {New England Journal of Medicine},
   title = {Statistical analysis plan for ``{L}ifting Universal Masking in 
Schools — Covid-19 Incidence among Students and Staff''},
   volume = {387},
   year = {2022},
}
